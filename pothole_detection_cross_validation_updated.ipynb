{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nx5oT_MsbgND"
   },
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FpGHI1hzZN7s",
    "partialCollapse": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from scipy import misc, ndimage\n",
    "from PIL import Image\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from keras.backend import sigmoid\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from tensorflow.keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "from keras.regularizers import l2, l1, l1_l2\n",
    "from keras.backend import manual_variable_initialization \n",
    "manual_variable_initialization(True)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import requests\n",
    "import json\n",
    "import glob\n",
    "from math import exp\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, RemoteMonitor, LambdaCallback, ReduceLROnPlateau, Callback, LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.image as mimg\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "import matplotlib.backends.backend_pdf\n",
    "import pandas as pd\n",
    "from scipy import misc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from numpy import savetxt\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from custom_callbacks import GetLearningRate\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE FUNCTIONS AND CLASSES\n",
    "def tg_sendtext(bot_message):\n",
    "    try:\n",
    "        bot_token = '1483800645:AAHybxYA3hLnwhegjU4wV6oeNpFPtfOcuh4'\n",
    "        bot_chatID = '1220107026'\n",
    "        send_text = 'https://api.telegram.org/bot' + bot_token + '/sendMessage?chat_id=' + bot_chatID + '&parse_mode=Markdown&text=' + bot_message\n",
    "\n",
    "        response = requests.get(send_text)\n",
    "        return response.json()\n",
    "    except:\n",
    "        print('Message Failed To Send')\n",
    "        pass\n",
    "\n",
    "def message_discord(message):\n",
    "    try:\n",
    "        channelID = \"777064829063135234\" # enable dev mode on discord, right-click on the channel, copy ID\n",
    "        botToken = \"Nzc3MDYzODcyMjgzNTQxNTE0.X69_IA.TtsQh9eVhxFfeakGkQLqn9UDAxw\"    # get from the bot page. must be a bot, not a discord app\n",
    "\n",
    "        baseURL = \"https://discordapp.com/api/channels/{}/messages\".format(channelID)\n",
    "        headers = { \"Authorization\":\"Bot {}\".format(botToken),\n",
    "                    \"User-Agent\":\"myBotThing (http://some.url, v0.1)\",\n",
    "                    \"Content-Type\":\"application/json\", }\n",
    "\n",
    "        POSTedJSON =  json.dumps ( {\"content\":message} )\n",
    "\n",
    "        r = requests.post(baseURL, headers = headers, data = POSTedJSON)\n",
    "    except:\n",
    "        print('Message Failed To Send')\n",
    "        pass\n",
    "\n",
    "def listdir_nohidden(path):\n",
    "    for f in os.listdir(path):\n",
    "        if not f.startswith('.'):\n",
    "            yield f\n",
    "\n",
    "def check_accuracy(mdl, X_new, y_new):\n",
    "    true = 0\n",
    "    Y_pred = mdl.predict(X_new)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    for j in range(len(y_new)):\n",
    "        if y_pred[j] == y_new[j]:\n",
    "            true += 1\n",
    "    accc = (true/len(y_new))*100\n",
    "    return accc\n",
    "\n",
    "def check_highest(folder_path1, train_max, max_acc, max_i):\n",
    "    temp_max_acc = [0]\n",
    "    temp_max_i = [0]\n",
    "    for i in os.listdir(folder_path1):\n",
    "        model = load_model(folder_path1 + i, custom_objects = {\"Activation\": Activation, \"lr\": lr})\n",
    "        acc = check_accuracy(model, X_new, y_new)\n",
    "        print('Model Accuracy =', str(acc) + '%')\n",
    "        if acc > 97.4:\n",
    "            os.remove(folder_path1 + i)\n",
    "            new_name = i.replace(\".h5\",\"\") + '_' + str(round(acc, 2)) + '.h5'\n",
    "            model.save('good_models/' + new_name)\n",
    "            temp_max_acc.append(acc)\n",
    "            temp_max_i.append(new_name)\n",
    "        else:\n",
    "            os.remove(folder_path1 + i)\n",
    "    max_acc = np.append(max_acc, temp_max_acc)\n",
    "    max_i = np.append(max_i, temp_max_i)\n",
    "    maxx = np.amax(max_acc)\n",
    "    index = np.where(max_acc == maxx)\n",
    "    if maxx > train_max:\n",
    "        train_max = maxx\n",
    "        print('New High Accuracy =', train_max)\n",
    "        mess = str(train_max)\n",
    "        tg_sendtext('New High Accuracy = ' + mess)\n",
    "        message_discord('New High Accuracy = ' + mess)\n",
    "    print('The highest Accuracy:', max_i[index], 'with', str(maxx) + '%')\n",
    "    return train_max, acc, max_acc, max_i\n",
    "\n",
    "def check_highest_custom(folder_path1, folder_path2, val):\n",
    "    for i in os.listdir(folder_path1):\n",
    "        model = load_model(folder_path1 + i, custom_objects = {\"Activation\": Activation, \"lr\": lr})\n",
    "        acc = check_accuracy(model, X_new, y_new)\n",
    "        if acc > val:\n",
    "            os.remove(folder_path1 + i)\n",
    "            new_name = str(round((random.uniform(0, 10) + random.uniform(0, 10)) * random.uniform(0, 100), 5)) + '_' + i.replace(\".h5\",\"\") + '.h5'\n",
    "            model.save(folder_path2 + new_name)\n",
    "        else:\n",
    "            os.remove(folder_path1 + i)\n",
    "            \n",
    "def check_highest_rename(folder_path1, folder_path2, val, X_new, Y_new):\n",
    "    for i in os.listdir(folder_path1):\n",
    "        model = load_model(folder_path1 + i, custom_objects = {\"Activation\": Activation, \"lr\": lr})\n",
    "        acc = check_accuracy(model, X_new, Y_new)\n",
    "        print('Model Name:', i)\n",
    "        print('Model Accuracy =', str(acc) + '%')\n",
    "        print('------------------------------------------------------------------------')\n",
    "        if acc > val:\n",
    "            os.remove(folder_path1 + i)\n",
    "            new_name = str(round((random.uniform(0, 10) + random.uniform(0, 10)) * random.uniform(0, 100), 5)) + '_' +i.replace(\".h5\",\"\") + '_' + str(round(acc, 2)) + '.h5'\n",
    "            model.save(folder_path2 + new_name)\n",
    "        else:\n",
    "            os.remove(folder_path1 + i)\n",
    "\n",
    "def data():\n",
    "    \n",
    "    class GetLearningRate(Callback):\n",
    "        def __init__(self, array):\n",
    "            self.array = array\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            lr = self.model.optimizer.lr\n",
    "            decay = self.model.optimizer.decay\n",
    "            iterations = self.model.optimizer.iterations\n",
    "            lr_with_decay = lr / (1. + decay * K.cast(iterations, K.dtype(decay)))\n",
    "            self.array.append(lr_with_decay)\n",
    "            print('Learning Rate = ', K.eval(lr_with_decay))\n",
    "    \n",
    "    x = np.load('dataset/x_conc.npy')\n",
    "    y = np.load('dataset/y_conc.npy')\n",
    "    x_val = np.load('dataset/X_val.npy')\n",
    "    y_val = np.load('dataset/y_val.npy')\n",
    "    return x, y, x_val, y_val\n",
    "\n",
    "def model_plot(model, file_name):\n",
    "    for i in model:\n",
    "        tf.keras.utils.plot_model(\n",
    "        model,\n",
    "        to_file=f\"model_graphs/{file_name}.png\",\n",
    "        show_shapes=True,\n",
    "        show_layer_names=True,\n",
    "        expand_nested=True,\n",
    "        dpi=96)\n",
    "        \n",
    "def plot_graph(model_history):\n",
    "    list_plots = []\n",
    "    for hist in model_history:\n",
    "        graph = plt.figure()\n",
    "        axis1 = graph.add_subplot(211)\n",
    "        axis1.plot(hist.history['accuracy'])\n",
    "        axis1.plot(hist.history['val_accuracy'])\n",
    "        axis1.title.set_text('Model Accuracy')\n",
    "        axis1.set_ylabel('Accuracy')\n",
    "        axis1.set_xlabel('Epoch')\n",
    "        axis1.legend(['Train', 'Test'], loc='upper left')\n",
    "        \n",
    "        axis2 = graph.add_subplot(212)\n",
    "        axis2.plot(hist.history['loss'])\n",
    "        axis2.plot(hist.history['val_loss'])\n",
    "        axis2.title.set_text('Model Loss')\n",
    "        axis2.set_ylabel('Loss')\n",
    "        axis2.set_xlabel('Epoch')\n",
    "        axis2.legend(['Train', 'Test'], loc='upper left')\n",
    "        graph.tight_layout()\n",
    "        plt.close()\n",
    "        list_plots.append(graph)\n",
    "    return list_plots\n",
    "\n",
    "def plot_combine(plots, accs, loss, lrs, beta1, beta2, name):\n",
    "    graph = plt.figure()\n",
    "    axis1 = graph.add_subplot(211)\n",
    "    axis1.plot(accs)\n",
    "    axis1.title.set_text('General Model Accuracy')\n",
    "    axis1.set_ylabel('Accuracy')\n",
    "    axis1.set_xlabel('Folds')\n",
    "\n",
    "    axis2 = graph.add_subplot(212)\n",
    "    axis2.plot(loss)\n",
    "    axis2.title.set_text('General Model Loss')\n",
    "    axis2.set_ylabel('Loss')\n",
    "    axis2.set_xlabel('Folds')\n",
    "    \n",
    "    graph.suptitle('General Metrics')\n",
    "    graph.tight_layout()\n",
    "    plt.close()\n",
    "    \n",
    "    graph1 = plt.figure()\n",
    "    axis = graph1.add_subplot(311)\n",
    "    axis.plot(lrs)\n",
    "    axis.title.set_text('Learning Rate')\n",
    "    axis.set_ylabel('Lr')\n",
    "    axis.set_xlabel('Epochs')\n",
    "    \n",
    "    axis3 = graph1.add_subplot(312)\n",
    "    axis3.plot(beta1)\n",
    "    axis3.title.set_text('Beta 1')\n",
    "    axis3.set_ylabel('beta1')\n",
    "    axis3.set_xlabel('Epochs')\n",
    "    \n",
    "    axis4 = graph1.add_subplot(313)\n",
    "    axis4.plot(beta2)\n",
    "    axis4.title.set_text('Beta 2')\n",
    "    axis4.set_ylabel('beta2')\n",
    "    axis4.set_xlabel('Epochs')\n",
    "    \n",
    "    graph1.suptitle('Lr, Beta1, Beta2')\n",
    "    graph1.tight_layout()\n",
    "    plt.close()\n",
    "\n",
    "    pdf = matplotlib.backends.backend_pdf.PdfPages(name)\n",
    "    fold = 1\n",
    "    for fig in plots:\n",
    "        fig.suptitle(f'Fold {fold}')\n",
    "        fig.tight_layout()\n",
    "        plt.close()\n",
    "        pdf.savefig(fig)\n",
    "        fold += 1\n",
    "    pdf.savefig(graph)\n",
    "    pdf.savefig(graph1)\n",
    "    pdf.close()\n",
    "\n",
    "class GetLearningRate(Callback):\n",
    "    def __init__(self, lr_array, beta1_array, beta2_array):\n",
    "        self.lr_array = lr_array\n",
    "        self.beta1_array = beta1_array\n",
    "        self.beta2_array = beta2_array\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        decay = self.model.optimizer.decay\n",
    "        lr = self.model.optimizer.lr\n",
    "        iters = self.model.optimizer.iterations\n",
    "        beta_1 = self.model.optimizer.beta_1\n",
    "        beta_2 = self.model.optimizer.beta_2\n",
    "        lr = lr * (1. / (1. + decay * K.cast(iters, K.dtype(decay))))\n",
    "        t = K.cast(iters, K.floatx()) + 1\n",
    "        lr_t = lr * (K.sqrt(1. - K.pow(beta_2, t)) / (1. - K.pow(beta_1, t)))\n",
    "        self.lr_array.append(lr_t)\n",
    "        self.beta1_array.append(beta_1)\n",
    "        self.beta2_array.append(beta_2)\n",
    "        print(f'lr: {lr_t}, beta_1: {beta_1}, beta_2: {beta_2}')\n",
    "\n",
    "def exp_decay(epoch):\n",
    "    k = 0.01\n",
    "    lrate = initial_lrate * exp(-k*epoch)\n",
    "    return lrate\n",
    "\n",
    "def get_lr_metric(optimizer):\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nx5oT_MsbgND"
   },
   "source": [
    "# **Pre-Processing (Dont run)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8rAxA1vcYu3",
    "outputId": "e450ff76-3198-4f34-8546-dd032e669d10",
    "partialCollapse": true
   },
   "outputs": [],
   "source": [
    "# PATHS\n",
    "root_path = 'pothole_detection'\n",
    "dataset = 'dataset'\n",
    "categories = ['pothole', 'no_pothole']\n",
    "test_categories = ['test_pothole', 'test_no_pothole']\n",
    "image_size = 120\n",
    "data = []\n",
    "data_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDc7zmzFZN70",
    "outputId": "2da23daa-4c0c-4b3e-a694-a75e8feaee2c",
    "partialCollapse": true
   },
   "outputs": [],
   "source": [
    "# LOAD TEST TRAIN VALUES\n",
    "cnt = 0\n",
    "for category in categories:\n",
    "    folder = os.path.join(dataset, category)\n",
    "    label = categories.index(category)\n",
    "    for img in os.listdir(folder):\n",
    "        try:\n",
    "            img_path = os.path.join(folder, img)\n",
    "            cnt+=1\n",
    "            print(category, cnt, label)\n",
    "            clear_output(wait = True)\n",
    "            img_array = cv2.imread(img_path)\n",
    "            img_array = cv2.resize(img_array, (image_size, image_size))\n",
    "            data.append([img_array, label])\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "\n",
    "for category in test_categories:\n",
    "    folder = os.path.join(dataset, category)\n",
    "    label = test_categories.index(category)\n",
    "    for img in os.listdir(folder):\n",
    "        try:\n",
    "            img_path = os.path.join(folder, img)\n",
    "            cnt+=1\n",
    "            print(category, cnt, label)\n",
    "            clear_output(wait = True)\n",
    "            img_array = cv2.imread(img_path)\n",
    "            img_array = cv2.resize(img_array, (image_size, image_size))\n",
    "            data_test.append([img_array, label])\n",
    "        except Exception as e:\n",
    "            print(str(e))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRt-Vfo_ZN72",
    "partialCollapse": true
   },
   "outputs": [],
   "source": [
    "# SPLIT TEST TRAIN VALUES\n",
    "np.random.shuffle(data)\n",
    "np.random.shuffle(data_test)\n",
    "\n",
    "X_old = []\n",
    "y_old = []\n",
    "X_old_test = []\n",
    "y_old_test = []\n",
    "\n",
    "for features, labels in data:\n",
    "    X_old.append(features)\n",
    "    y_old.append(labels)\n",
    "\n",
    "X_train = np.array(X_old)\n",
    "y_train = np.array(y_old)\n",
    "\n",
    "for features, labels in data_test:\n",
    "    X_old_test.append(features)\n",
    "    y_old_test.append(labels)\n",
    "\n",
    "X_test = np.array(X_old_test)\n",
    "y_test = np.array(y_old_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "partialCollapse": true
   },
   "outputs": [],
   "source": [
    "# LOAD VALIDATION VALUES\n",
    "root_path = ''\n",
    "dataset = 'dataset'\n",
    "categories = ['pothole', 'no_pothole']\n",
    "image_size = 120\n",
    "\n",
    "folder_path = 'validation'\n",
    "data_new = []\n",
    "\n",
    "cnt = 0\n",
    "for category in categories:\n",
    "    folder_new = os.path.join(dataset, folder_path, category)\n",
    "    label_new = categories.index(category)\n",
    "    for img_new in os.listdir(folder_new):\n",
    "        img_path_new = os.path.join(folder_new, img_new)\n",
    "        cnt+=1\n",
    "        print(category, cnt, label_new)\n",
    "        clear_output(wait = True)\n",
    "        img_array_new = cv2.imread(img_path_new)\n",
    "        img_array_new = cv2.resize(img_array_new, (image_size, image_size))\n",
    "        data_new.append([img_array_new, label_new])\n",
    "\n",
    "np.random.shuffle(data_new)\n",
    "X_old = []\n",
    "y_old = []\n",
    "\n",
    "for features_new, labels_new in data_new:\n",
    "    X_old.append(features_new)\n",
    "    y_old.append(labels_new)\n",
    "\n",
    "X_new = np.array(X_old)\n",
    "y_new = np.array(y_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "partialCollapse": true
   },
   "outputs": [],
   "source": [
    "# SAVE NP ARRAYS\n",
    "# np.save('dataset/X_train', X_train)\n",
    "# np.save('dataset/X_test', X_test)\n",
    "# np.save('dataset/y_train', y_train)\n",
    "# np.save('dataset/y_test', y_test)\n",
    "np.save('dataset/X_val', X_new)\n",
    "np.save('dataset/y_val', y_new)\n",
    "\n",
    "x = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "np.save('dataset/x_conc', x)\n",
    "np.save('dataset/y_conc', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nx5oT_MsbgND"
   },
   "source": [
    "# **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "partialCollapse": true
   },
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "x = np.load('dataset/x_conc.npy')\n",
    "y = np.load('dataset/y_conc.npy')\n",
    "X_new = np.load('dataset/X_val.npy')\n",
    "y_new = np.load('dataset/y_val.npy')\n",
    "X_new = X_new / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nx5oT_MsbgND"
   },
   "source": [
    "# **Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "partialCollapse": true
   },
   "outputs": [],
   "source": [
    "# DEFINE TRAIN_CROSS_VAL MODEL\n",
    "def train_cross_val(input_layer_neurons_1_val, input_layer_neurons_2_val, input_layer_neurons_3_val, first_layer_neurons, mes):\n",
    "    n_folds = 10\n",
    "    pat = 3\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=1)\n",
    "    model_checkpoint_best = ModelCheckpoint('checkpoints/' + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\") + '.h5', verbose=1, save_best_only=True)\n",
    "    log_dir = \"logs/fit/\" + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\")\n",
    "    logs = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    ### Model ###\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(input_layer_neurons_1_val, input_shape = x.shape[1:], kernel_size = 3, strides = 1, activation = 'relu', padding = 'same'))\n",
    "    model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(input_layer_neurons_2_val, kernel_size = 3, strides = 1, activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Conv2D(input_layer_neurons_3_val, kernel_size = 3, strides = 1, activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Flatten())\n",
    "x\n",
    "    model.add(Dense(first_layer_neurons, activation = 'relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "    model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    model_histories = []\n",
    "    \n",
    "    print('training')\n",
    "    print('Fold Num =', end=' ')\n",
    "    for i in range(n_folds):\n",
    "        print(i+1, end=' ')\n",
    "        model_checkpoint = ModelCheckpoint('temp_checkpoint/' + \"model_{epoch:02d}\" + '_' + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\") + '.h5', verbose=0, save_best_only=False, monitor='val_loss')\n",
    "        train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2,\n",
    "                                                   random_state = np.random.randint(1,1000, 1)[0])\n",
    "        \n",
    "        train_x = train_x / 255\n",
    "        test_x = test_x / 255\n",
    "        \n",
    "        history = model.fit(train_x, train_y, epochs=10, callbacks=[early_stopping, model_checkpoint, model_checkpoint_best], verbose=1, validation_split=(0.2))\n",
    "        model_histories.append(history)\n",
    "        \n",
    "        scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "        \n",
    "        acc_per_fold.append(scores[1])\n",
    "        loss_per_fold.append(scores[0])\n",
    "        \n",
    "        tg_sendtext(f'Finished fold number {i+1} of {mes} with accuracy = {scores[1]*100}%')\n",
    "        message_discord(f'Finished fold number {i+1} of {mes} with accuracy = {scores[1]*100}%')\n",
    "        model.save('checkpoints/' + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\") + '.h5')\n",
    "        check_highest_custom('temp_checkpoint/', 'checkpoints/', 97.4)\n",
    "    print('Finished')\n",
    "    return model_histories, acc_per_fold, loss_per_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nx5oT_MsbgND"
   },
   "source": [
    "# **Auto Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "partialCollapse": true
   },
   "outputs": [],
   "source": [
    "# CREAT NEURON VALUES\n",
    "input_layer_neurons_1 = [32, 64, 128]\n",
    "input_layer_neurons_2 = [32, 64, 128]\n",
    "input_layer_neurons_3 = [32, 64, 128]\n",
    "first_layer_neurons = [128, 256, 512]\n",
    "res = [[i, j, k, l]\n",
    "       for i in input_layer_neurons_1\n",
    "       for j in input_layer_neurons_2\n",
    "       for k in input_layer_neurons_3\n",
    "       for l in first_layer_neurons\n",
    "      ]\n",
    "new_res = []\n",
    "for i in res:\n",
    "    if i[0] > i[1]:\n",
    "        new_res.append(i)\n",
    "    elif i[1] > i[2]:\n",
    "        new_res.append(i)\n",
    "for i in new_res:\n",
    "    res.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideInput": false,
    "hideOutput": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AUTO TRAINER\n",
    "maxx = 0\n",
    "count = 1\n",
    "maxx_acc = []\n",
    "maxx_i = []\n",
    "for i in res:\n",
    "    print(count)\n",
    "    neurons = [i[0], i[1], i[2], i[3]]\n",
    "    print('Neurons =', i[0], i[1], i[2], i[3])\n",
    "    message = 'model ' + str(count)\n",
    "    tg_sendtext('Started training ' + message + ' with neurons = ' + str(neurons))\n",
    "    message_discord('Started training ' + message + ' with neurons = ' + str(neurons))\n",
    "    hist, accuracies, losses = train_cross_val(i[0], i[1], i[2], i[3], message)\n",
    "    maxx, accuracy, maxx_acc, maxx_i = check_highest('checkpoints/', maxx, maxx_acc, maxx_i)\n",
    "    plot_lists = plot_graph(hist)\n",
    "    plot_combine(plot_lists, accuracies, losses, 'model_graphs/' + str(neurons) + '_' + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\") + '_' + str(round(accuracy, 2)) + '.pdf')\n",
    "    message = message + ' ' + str([i[0], i[1], i[2], i[3]]) + ' with final accuracy = ' + str(accuracy)\n",
    "    tg_sendtext('Finished training ' + message)\n",
    "    message_discord('Finished training ' + message)\n",
    "    count += 1\n",
    "    print('------------------------------------------------------------------------')\n",
    "    tg_sendtext('------------------------------------------------------------------------')\n",
    "    message_discord('------------------------------------------------------------------------')\n",
    "print('AHHHHHHH')\n",
    "tg_sendtext(f'Training Completed | Highest Accuracy = {maxx}')\n",
    "message_discord(f'Training Completed | Highest Accuracy = {maxx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nx5oT_MsbgND"
   },
   "source": [
    "# **Tester**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideOutput": true
   },
   "outputs": [],
   "source": [
    "# TESTER \n",
    "n_folds = 10\n",
    "tg_sendtext(f'Training Model')\n",
    "message_discord(f'Training Model')\n",
    "pat = 3\n",
    "lr_arr = []\n",
    "beta1_arr = []\n",
    "beta2_arr = []\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=1)\n",
    "model_checkpoint_best = ModelCheckpoint('checkpoints/' + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\") + '.h5', verbose=1, save_best_only=True)\n",
    "log_dir = \"logs/fit/\" + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\")\n",
    "logs = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "lr = GetLearningRate(lr_arr, beta1_arr, beta2_arr)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, input_shape = x.shape[1:], kernel_size = 3, strides = 1, activation = 'relu', padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
    "model.add(Dropout(0.2071809982680866))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size = 3, strides = 1, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
    "model.add(Dropout(0.04612987161018767))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size = 3, strides = 1, activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
    "model.add(Dropout(0.10471119809697471))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation = 'relu', kernel_initializer = 'HeNormal'))\n",
    "model.add(Dropout(0.28309374387487923))\n",
    "\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(lr=0.001)\n",
    "lr_metric = get_lr_metric(optimizer)\n",
    "model.compile(optimizer = optimizer, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy', lr_metric])\n",
    "\n",
    "model_histories = []\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "factor = 0.7\n",
    "\n",
    "for i in range(n_folds):\n",
    "    print(f'Fold Number = {i+1}')\n",
    "    factor = 0.7*((1.036311)**i)\n",
    "    rlrop = ReduceLROnPlateau(monitor='val_loss', factor=factor, patience=3, verbose=1)\n",
    "    model_checkpoint = ModelCheckpoint('temp_checkpoint/' + \"model_{epoch:02d}\" + '_' + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\") + '.h5', verbose=0, save_best_only=False, monitor='val_loss')\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state = np.random.randint(1,1000, 1)[0])\n",
    "\n",
    "    train_x = train_x / 255\n",
    "    test_x = test_x / 255  \n",
    "\n",
    "    history = model.fit(train_x, train_y, epochs=10, callbacks=[early_stopping, model_checkpoint, model_checkpoint_best, logs, lr, rlrop], verbose=1, validation_split=(0.2))\n",
    "    model_histories.append(history)\n",
    "\n",
    "    scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "\n",
    "    acc_per_fold.append(scores[1])\n",
    "    loss_per_fold.append(scores[0])\n",
    "\n",
    "    print(f'Fold {i+1} Accuracy = {scores[1]*100}%')\n",
    "\n",
    "    tg_sendtext(f'Finished fold number {i+1} with accuracy = {scores[1]*100}%')\n",
    "    message_discord(f'Finished fold number {i+1} with accuracy = {scores[1]*100}%')\n",
    "    model.save('checkpoints/' + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\") + '.h5')\n",
    "    check_highest_rename('temp_checkpoint/', 'good_models/', 97.6, X_new, y_new)\n",
    "score, accuracy = model.evaluate(X_new, y_new, verbose=0)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "learning_rates = lr.lr_array\n",
    "beta_1 = lr.beta1_array\n",
    "beta_2 = lr.beta2_arr\n",
    "plot_lists = plot_graph(model_histories)\n",
    "plot_combine(plot_lists, acc_per_fold, loss_per_fold, beta_1, beta_2, learning_rates,'model_graphs/' + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\") + '_' + str(round(accuracy, 2)) + '.pdf')\n",
    "\n",
    "check_highest_rename('temp_checkpoint/', 'good_models/', 97.6, X_new, y_new)\n",
    "check_highest_rename('checkpoints/', 'good_models/', 97.6, X_new, y_new)\n",
    "check_highest_rename('saved_models/', 'good_models/', 97.6, X_new, y_new)\n",
    "tg_sendtext(f'Finished training Model with accuracy = {accuracy}')\n",
    "message_discord(f'Finished training Model with accuracy = {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "partialCollapse": true
   },
   "outputs": [],
   "source": [
    "# TESTER\n",
    "n_folds = 10\n",
    "pat = 3\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=1)\n",
    "model_checkpoint_best = ModelCheckpoint('checkpoints/' + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\") + '.h5', verbose=1, save_best_only=True)\n",
    "model_checkpoint = ModelCheckpoint('temp_checkpoint/' + \"model_{epoch:02d}\" + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\") + '.h5', verbose=0, save_best_only=False, monitor='val_loss')\n",
    "rlrop = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=2)\n",
    "log_dir = \"logs/fit/\" + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\")\n",
    "logs = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "### Model ###\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, input_shape = X_train.shape[1:], kernel_size = 3, strides = 1, activation = 'relu', padding = 'same', kernel_initializer = 'random_uniform'))\n",
    "model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size = 3, strides = 1, activation = 'relu', kernel_initializer = 'random_uniform'))\n",
    "model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation = 'relu', kernel_initializer = 'random_uniform'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(2, activation = 'softmax', kernel_initializer = 'random_uniform'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, callbacks=[early_stopping, model_checkpoint, model_checkpoint_best, logs, rlrop], verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "scores = model.evaluate(X_new, y_new, verbose=0)\n",
    "\n",
    "model.save('checkpoints/' + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\") + '.h5')\n",
    "print('Finished')\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nx5oT_MsbgND"
   },
   "source": [
    "# **Hyperas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "partialCollapse": true
   },
   "outputs": [],
   "source": [
    "# CREATING HYPERAS MODEL\n",
    "\n",
    "def create_model(x, y, x_val, y_val):\n",
    "    n_folds = 10\n",
    "    tg_sendtext(f'Training Model')\n",
    "    message_discord(f'Training Model')\n",
    "    pat = 3\n",
    "    arr = []\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=0)\n",
    "    model_checkpoint_best = ModelCheckpoint('checkpoints/' + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\") + '.h5', verbose=0, save_best_only=True)\n",
    "    log_dir = \"logs/fit/\" + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\")\n",
    "    logs = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    lr = GetLearningRate(arr)\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D({{choice([32, 64])}}, input_shape = x.shape[1:], kernel_size = 3, strides = 1, activation = 'relu', padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
    "    model.add(Dropout({{uniform(0, 0.5)}}))\n",
    "\n",
    "\n",
    "    model.add(Conv2D({{choice([32, 64])}}, kernel_size = 3, strides = 1, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
    "    model.add(Dropout({{uniform(0, 0.5)}}))\n",
    "\n",
    "\n",
    "    model.add(Conv2D({{choice([32, 64, 128])}}, kernel_size = 3, strides = 1, activation = 'relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size = 2, strides = 2))\n",
    "    model.add(Dropout({{uniform(0, 0.5)}}))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense({{choice([128, 256, 512])}}, activation = 'relu'))\n",
    "    model.add(Dropout({{uniform(0, 0.5)}}))\n",
    "\n",
    "    model.add(Dense(2, activation = 'softmax'))\n",
    "                    \n",
    "    model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    model_histories = []\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "    \n",
    "    for i in range(n_folds):\n",
    "        model_checkpoint = ModelCheckpoint('temp_checkpoint/' + \"model_{epoch:02d}\" + '_' + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\") + '.h5', verbose=0, save_best_only=False, monitor='val_loss')\n",
    "        train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2,\n",
    "                                                   random_state = np.random.randint(1,1000, 1)[0])\n",
    "        \n",
    "        train_x = train_x / 255\n",
    "        test_x = test_x / 255\n",
    "        \n",
    "        history = model.fit(train_x, train_y, epochs=10, callbacks=[early_stopping, model_checkpoint, model_checkpoint_best, logs, lr], verbose=2, validation_split=(0.2))\n",
    "        model_histories.append(history)\n",
    "        \n",
    "        scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "        \n",
    "        acc_per_fold.append(scores[1])\n",
    "        loss_per_fold.append(scores[0])\n",
    "        \n",
    "        print(f'Fold {i+1} Accuracy = {scores[1]*100}%')\n",
    "        \n",
    "        tg_sendtext(f'Finished fold number {i+1} with accuracy = {scores[1]*100}%')\n",
    "        message_discord(f'Finished fold number {i+1} with accuracy = {scores[1]*100}%')\n",
    "        model.save('checkpoints/' + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\") + '.h5')\n",
    "        check_highest_rename('temp_checkpoint/', 'checkpoints/', 97, x_val, y_val)\n",
    "    score, accuracy = model.evaluate(x_val, y_val, verbose=0)\n",
    "    print('Accuracy:', accuracy)\n",
    "\n",
    "    learning_rates = lr.array\n",
    "    plot_lists = plot_graph(model_histories)\n",
    "    plot_combine(plot_lists, acc_per_fold, loss_per_fold, learning_rates,'model_graphs/' + (datetime.datetime.utcnow()+datetime.timedelta(hours=5.5)).strftime(\"%d-%m-%Y--%H.%M.%S\") + '_' + str(round(accuracy, 2)) + '.pdf')\n",
    "    \n",
    "    check_highest_rename('temp_checkpoint/', 'good_models/', 97, x_val, y_val)\n",
    "    check_highest_rename('checkpoints/', 'good_models/', 97, x_val, y_val)\n",
    "    check_highest_rename('saved_models/', 'good_models/', 97, x_val, y_val)\n",
    "    tg_sendtext(f'Finished training Model with accuracy = {accuracy}')\n",
    "    message_discord(f'Finished training Model with accuracy = {accuracy}')\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideOutput": true,
    "partialCollapse": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# HYPERAS OPTIM\n",
    "best_run, best_model = optim.minimize(model=create_model, data=data, algo=tpe.suggest, max_evals=10, trials=Trials(), \n",
    "                                      notebook_name='pothole_detection_cross_validation_updated', \n",
    "                                      functions=[check_highest_rename, tg_sendtext, message_discord, check_accuracy, plot_graph, plot_combine])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
